---
title: "Atlantis biomass distributions with sdmTMB - lat, lon, depth"
author: "Alberto Rovellini"
date: "3/14/2022"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

# Purpose

This is a template for fitting `sdmTMB` to GOA bottom trawl data. For each stage of species, we fit an `sdmTMB` model to AKFIN CPUE data (manually calculated from catch and effort information, see `catch_to_cpue_DFO.Rmd`) to predict CPUE in kg per km$^{2}$ onto a regular grid. This predicted CPUE is then scaled up to box area in Atlantis, to get biomass per box and proportion of the total by box, which is what we need to initialize Atlantis. This same code will be run for biomass pool species too, except for those we will not split into juveniles and adults.

Catch of juveniles and adults for each species is calculated in `catch_to_cpue_DFO.Rmd` based on length composition data from the bottom trawl, length at maturity, and a length-weight relationship. 

This workflow is based on the following assumptions:

1. We use lat and lon (converted from degrees to km) and depth as predictors. We do not use environmental covariates as predictors because we are not attempting to explain why species are distributed the way they are, but rather we are trying to have sensible generic distributions over the model domain throughout the study period (2003-2019).
2. We predict over a regular 10 km grid. After some testing with predicting over a grid of 1 point per Atlantis box, I chose to use a regular grid because: (1) an average value of lat and lon, such as a centroid, is difficult to calculate for some of the boxes with crescent shapes; and (2) some boxes are placed over areas where depth changes greatly (the GOA bathymetry is complex), and the inside points may fall inside or near a deeper/shallower area withih a certain box. While the Atlantis box itself has a constant depth, the nearest node of the SPDE mesh may have been near such deeper/shallower area, thus skewing the estimate of the biomass index for that particular box.
3. We are not so interested in accurate predictions for any one year, but rather in representative means of where the fish has been over the last few decades. This code runs a temporal model and takes averages of the estimates at the end.

This workflow does not include Alaska. 

Note: This data extends deeper than 1000 m. We may decide to discard the data points deeper than 1000 m for consistency with the RACE-GAP data, or we may just keep them all for the sake of sample size. Sets deeper than 1000 m are <1% of the sets, so probably hardly any difference either way.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is DFO data from [here](https://open.canada.ca/data/en/dataset/a278d1af-d567-4964-a109-ae1e84cbd24a). 
Note: we use ending latitude and longitude values for each tow.

Take a quick look at the data spatially.
```{r, fig.width = 12, fig.height = 12}
ggplot()+
  geom_point(data = dfo_data, aes(lon, lat, colour = log1p(biom_kgkm2)), size = 1.5)+
  scale_colour_viridis_c()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(dfo_data$lon),max(dfo_data$lon)), ylim=c(min(dfo_data$lat),max(dfo_data$lat)))+
  theme_minimal()+
  facet_wrap(~year, ncol = 3)+
  labs(title = paste(dfo_data$name,"CPUE from DFO bottom trawl survey, stage:", dfo_data$stage, sep = " "))
```

Take a quick look at time series of total CPUE from raw data
```{r, fig.align="center"}
biom_year <- dfo_data %>% group_by(year) %>% summarise(biom = sum(log1p(biom_kgkm2)))

ggplot(biom_year, aes(year, log(biom)))+
  geom_point()+
  geom_path()+
  theme_minimal()+
  labs(title = paste(dfo_data$name,"total GOA CPUE from bottom \n trawl survey - stage:", dfo_data$stage, sep = " "))
```
The above is for the whole area. 

# Add zeroes for hauls with no catch

We need to add empty hauls (i.e., hauls where the catch for a given species is 0). 
```{r}
data_hauls <- levels(factor(dfo_data$hauljoin))
zero_hauls <- setdiff(levels(factor(effort_all$Haul.Join.ID)), data_hauls) # assuming that if there are no records from a haul, the catch in that haul was 0 for this species

# make a data frame to bind by row
zero_catches <- effort_all %>% filter(Haul.Join.ID %in% zero_hauls) %>% 
  select(Survey.Year, Haul.Join.ID, End.latitude, End.longitude, Bottom.depth..m.) %>% 
  mutate(species_code = rep(NA, length(Survey.Year)),
         name = rep(NA, length(Survey.Year)),
         stage = rep(NA, length(Survey.Year)),
         biom_kgkm2 = rep(0, length(Survey.Year))) %>%
  set_names(names(dfo_data))

# attach by row to race_data
dfo_data <- rbind(dfo_data, zero_catches)
# ditch hauls with empty lat or lon
dfo_data <- dfo_data %>% filter(!is.na(lat) | !is.na(lon))
# and with NA depths
dfo_data <- dfo_data %>% filter(!is.na(depth))
```

# sdmTMB

## Create spatial mesh

### Transform coordinates

The first step here will be to go from the native lat lon coordinates of the bottom trawl data to projected coordinates. Here we use the custom projection that is used by the Atlantis geometry "+proj=tmerc +lat_0=50 +lon_0=-154 +lat_1=55 +lat_2=65 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs". Note that this is still WGS84. May be worth comparing with NAD83. We used WGS84 because the Cehckwinding code did not seem to cope with NAD83. 

Read in the Atlantis BGM, then turn the bottom trawl data to an sf object, reproject it, and then turn it back to coordinates.
```{r}
dfo_data_sf <- dfo_data %>% st_as_sf(coords = c("lon", "lat"), crs = 4326) %>% st_transform(crs = atlantis_bgm$extra$projection)

# now extract coordinates, and divide by 1000 to transform m to km
dfo_coords <- dfo_data_sf %>% st_coordinates() %>% data.frame() %>% mutate(x=X/1000,y=Y/1000) %>% select(-X,-Y)

# turn sf back to a data frame
dfo_data <- dfo_data_sf %>% st_set_geometry(NULL) %>% data.frame(dfo_coords)
```

Adding an `sf` object of the coastline to incorporate in the mesh.
```{r}
data_bbox <- dfo_data_sf %>% st_bbox()
coast_mesh <- coast_sf %>% st_transform(crs=atlantis_bgm$extra$projection) %>%
  st_crop(data_bbox)
```

Using the "cutoff" argument, instead of predefining the number of points. This will help with AFSC vs DFO (use the same distance), also it does not depend on the random seed or on the order of the data.

**Note:** SPDE = Stochastic Partial Differential Equations approach. Some material can be found [here](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:spde), but basically it is a way of calculating the position of the mesh knots. 

We take land barriers into account when building the mesh (see `sdmTMB` documentation).
```{r}
dfo_spde <- make_mesh(dfo_data, c("x", "y"), cutoff = 20, type = "cutoff")

# add barrier
dfo_spde <- add_barrier_mesh(
  spde_obj = dfo_spde,
  barrier_sf = coast_mesh,
  range_fraction = 0.1,
  proj_scaling = 1000, # data km but projection m
  plot = TRUE
)

dfo_spde$mesh$n
```

Check out the distribution of the biomass response variable.
```{r}
hist(dfo_data$biom_kgkm2, breaks = 30)
```

```{r}
hist(log1p(dfo_data$biom_kgkm2), breaks = 30)
```

Proportion of zeroes in percentage.
```{r}
length(which(dfo_data$biom_kgkm2 == 0))/nrow(dfo_data)*100
```

## Space, time, and depth model.

Fitting a model with a smooth term for depth. Using 3 knots for the smooth, after testing k=5 (k=3 seems to return more sensible predictions at depth for several species). As a note, I am not scaling depth here. The reason is that depth has a different range in the data and the prediction grid, and thus scaled values have different meaning between the two.

**Model type**: We use a Tweedie model with a log link, as a relatively widely adopted option for similar survey data. 
```{r, results = FALSE}
start.time <- Sys.time()

m_depth <- sdmTMB(
    data = dfo_data, 
    formula = biom_kgkm2 ~ 0 + s(depth, k = 3) + as.factor(year), 
    mesh = dfo_spde, 
    time = "year", 
    spatial = 'on',
    spatiotemporal = 'iid', # spatiotemporal random fields independent and identically distributed
    reml = TRUE,
    anisotropy = FALSE,
    silent = FALSE,
    family = tweedie(link = "log"))

end.time <- Sys.time()
time.taken_m_depth <- end.time - start.time
time.taken_m_depth
```

Rerun with extra optimization steps in case of gradient > 0.001. 
```{r, results = FALSE}
if(abs(max(m_depth$gradients))>0.001){
  
  m_depth <- sdmTMB(
    data = dfo_data, 
    formula = biom_kgkm2 ~ 0 + s(depth, k = 3) + as.factor(year), 
    mesh = dfo_spde, 
    time = "year", 
    spatial = 'on',
    spatiotemporal = 'iid', # spatiotemporal random fields independent and identically distributed
    reml = TRUE,
    anisotropy = FALSE,
    silent = FALSE,
    control = sdmTMBcontrol(nlminb_loops = 2, newton_loops = 3),
    family = tweedie(link = "log"))
  
}
```

Check information on model convergence. From [the nlminb help page](https://rdrr.io/r/stats/nlminb.html) we know that an integer 0 indicates succesful convergence. Additional information on convergence can be checked with m_depth\$model\$message. According to the original [PORT optimization documentation](https://web.archive.org/web/20070203144320/http://netlib.bell-labs.com/cm/cs/cstr/153.pdf), "Desirable return codes are 3, 4, 5, and sometimes 6".  
```{r}
if(m_depth$model$convergence == 0){print("The model converged.")} else {print("Check convergence issue.")}
m_depth$model$message # convergence message
max(m_depth$gradients) # maximum gradient component 
tidy(m_depth, effects = 'ran_pars') %>% filter(term=='range') %>% pull(estimate) # Matérn range
```

The Matérn range is a parameter that indicates the distance at which data points become essentially independent. A large range means that spatial autocorrelation decays more slowly with distance. A smaller range will require more knots. A range larger than the cutoff argument should be sensible.

Check out model residuals. Note that this is a rough check, the package authors recommend to use DHARMA residuals (see approach [here](https://pbs-assess.github.io/sdmTMB/articles/residual-checking.html)).
```{r}
dfo_data$resids <- residuals(m_depth) # randomized quantile residuals
hist(dfo_data$resids)
```

And QQ plot.
```{r}
qqnorm(dfo_data$resids)
abline(a = 0, b = 1)
```

Plot the response curve from the depth smooth term.
```{r}
plot_smooth(m_depth, ggplot = T)
```

Finally, plot the residuals in space. If residuals are constantly larger/smaller in some of the areas, it may be sign that the model is biased and it over/underpredicts consistently for some areas. Residuals should be randomly distributed in space. 

For visualization purposes, multiply the coordinates by 1000 in the sf object to restore the correct scale.
```{r, fig.width = 12, fig.height=12}
dfo_sf <- dfo_data %>% mutate(x = x*1000, y = y*1000) %>% st_as_sf(coords = c(x = "x", y = "y"), crs = atlantis_bgm$extra$projection) #%>% st_transform(crs = atlantis_bgm$extra$projection) # turn to spatial object

coast_sf <- coast_sf %>% st_transform(crs = atlantis_bgm$extra$projection)

ggplot()+
  geom_sf(data = dfo_sf, aes(color = resids, alpha = .8))+
  scale_color_viridis()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+
  theme_minimal()+
  labs(title = paste(dfo_data$name,"model residuals in space - stage:", dfo_data$stage, sep = " "))+
  facet_wrap(~year, ncol = 3)
```

# Predictions from SDM

Take a grid (which must contain information on the predictors we used to build the model) and predict the biomass index over such grid based on the predictors. 

1. The grid is currently a regular grid with 10-km cell size, but 10 km might not be enough to get prediction points in all boxes - especially for a couple very small and narrow boxes at the western end of the model domain. Revisit this if necessary, but a finer mesh could be difficult to justify compared to the density of the survey data. 
2. The grid covers the entire Atlantis model domain, including the non-dynamic boundary boxes (deeper than 1000 m). Predictions for Alaska boxes will not be used here.

Read in the Atlantis prediction grid (10 km) modified in `Atlantis_grid_covars.R` (code not included here).
```{r}
atlantis_boxes <- atlantis_bgm %>% box_sf()
```

**Important:** depth in the RACE data is a positive number. Depth in the prediction grid we obtained from the ETOPO rasters is a negative number. When we use depth as predictor for in our regular grid, make sure depth is a positive number for consistency with the model variable, or else everything will be upside-down. This was done in the script that produces the prediction grid, so depth is **positive**. 
```{r}
atlantis_grid <- atlantis_grid_template

paste("Positive depths are:", length(which(atlantis_grid$depth>0)), "out of:", nrow(atlantis_grid_depth), sep = " ") # Write out a check that depths are positive (few negatives are OK - they are on land - I'll fix it but it should not matter as island boxes will be boundary boxes in Atlantis so predictions will not matter for those)

# add year column
all_years <- levels(factor(dfo_data$year))

atlantis_grid <- atlantis_grid[rep(1:nrow(atlantis_grid), length(all_years)),]
atlantis_grid$year <- as.integer(rep(all_years, each = nrow(atlantis_grid_depth)))
```

Visualize the prediction grid.
```{r}
coast_tmp <- map("worldHires", regions = c("Canada", "USA"), plot = FALSE, fill = TRUE)
coast_tmp <- coast_tmp %>% st_as_sf() %>% st_transform(crs = atlantis_bgm$extra$projection)

atlantis_grid %>% filter(year == 2003) %>%
  st_as_sf(coords = c("x", "y"), crs = atlantis_bgm$extra$projection) %>%
  ggplot()+
  geom_sf(size = 0.1)+
  geom_sf(data = coast_tmp)+
  coord_sf(xlim = c(-1160825.0,1799175.0), ylim = c(290420.6, 1799175.0))+ # -1160825.0   290420.6  1799175.0  1170420.6 
  theme_minimal()+
  labs(title = "Prediction grid")
```

Transform the coordinates, divide by 1000 to turn from m to km for consistency with the data.
```{r}
atlantis_grid <- atlantis_grid %>% mutate(x = x/1000, y = y/1000)
```

Make SDM predictions onto new data from depth model. **Back-transforming here**
```{r}
predictions_dfo <- predict(m_depth, newdata = atlantis_grid, return_tmb_object = TRUE)
atlantis_grid$estimates <- exp(predictions_dfo$data$est) #Back-transforming here

atlantis_grid_sf <- atlantis_grid %>% mutate(x=x*1000,y=y*1000) %>% st_as_sf(coords = c("x", "y"), crs = atlantis_bgm$extra$projection) # better for plots, multiplying the coordinates by 1000 for visualisation
```

Plotting only Canada.
```{r,  fig.width = 12, fig.height = 12}
ggplot()+
  geom_sf(data = subset(atlantis_boxes, box_id > 91), aes(fill = NULL))+
  geom_sf(data = subset(atlantis_grid_sf, box_id > 91), aes(color=log1p(estimates)))+ # taking the log for visualisation
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+  
  scale_color_viridis(name = expression(paste("Log(CPUE) kg ", km^-2)))+
  theme_minimal()+
  labs(title = paste(dfo_data$name,"predicted CPUE - stage:", dfo_data$stage, sep = " "))+
  facet_wrap(~year, ncol = 3)
```

Attribute the predictions to their respective Atlantis box, so that we can take box averages.
```{r}
atlantis_grid_means <- atlantis_grid %>% group_by(year, box_id) %>%
  summarise(mean_estimates = mean(estimates, na.rm = TRUE)) %>% ungroup() 

# join this with the box_sf file

predictions_by_box <- atlantis_boxes %>% inner_join(atlantis_grid_means, by = "box_id")
```

See estimates per box for all years. Silence boundary boxes as they they do not need predictions. 
```{r, fig.width = 12, fig.height = 12}
predictions_by_box <- predictions_by_box %>% rowwise() %>% mutate(mean_estimates = ifelse(isTRUE(boundary), NA, mean_estimates))

ggplot()+
  geom_sf(data = predictions_by_box[predictions_by_box$box_id> 91,], aes(fill = log1p(mean_estimates)))+ # taking the log for visualisation
  scale_fill_viridis(name = expression(paste("Log(CPUE) kg ", km^-2)))+
  theme_minimal()+
  geom_sf(data = coast_sf, colour = "black", fill = "grey80")+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+  
  facet_wrap(~year, ncol = 3)+
  labs(title = paste(dfo_data$name, "mean predicted CPUE by Atlantis box - stage:", dfo_data$stage, sep = " "))
```

Plot the raw data again for comparison.
```{r, fig.width = 12, fig.height = 12}
ggplot()+
  geom_sf(data = dfo_data_sf, aes(colour = log1p(biom_kgkm2)), size = 1.5, alpha = .5)+
  scale_colour_viridis_c(name = expression(paste("Log(CPUE) kg ", km^-2)))+
  geom_sf(data = coast_sf, colour = "black", fill = "grey80")+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+  
  theme_minimal()+
  facet_wrap(~year, ncol = 3)+
  labs(title = paste(dfo_data$name,"CPUE from DFO bottom trawl survey - stage:", dfo_data$stage, sep = " "))
```

Have a look at CPUE by depth. This is rough and quick, keep in mind that most tows happen shallower than 300 m, so the sample is not equal between depths.
```{r}
ggplot(data = dfo_data, aes(x = depth, y = log1p(biom_kgkm2)))+
  geom_point()+
  theme_minimal()+
  labs(title = "CPUE by depth")
```

Plot data and predictions distributions. These are the data.
```{r}
ggplot(data = dfo_data, aes(x = log1p(biom_kgkm2)))+
  geom_histogram(colour = "black", fill = 'grey80', bins = 30)+
  theme_minimal()
```

And these are the predictions over the 10 km grid.
```{r}
ggplot(data = atlantis_grid, aes(x = log1p(estimates)))+
  geom_histogram(colour = "black", fill = 'grey80', bins = 30)+
  theme_minimal()
```

# Mean predictions for the study period

Now calculate means of the predictions for the entire study period. Doing it by taking 2003-2019 averages for each Atlantis box.
```{r, fig.width = 10, fig.height = 5}
means_all_years <- predictions_by_box %>% group_by(box_id, area, boundary) %>% summarise(all_years_kgkm2 = mean(mean_estimates)) %>% ungroup()

ggplot()+
  geom_sf(data = means_all_years[means_all_years$box_id > 91,], aes(fill = log1p(all_years_kgkm2)))+ # log for visualisation
  scale_fill_viridis(name = expression(paste("Log(CPUE) kg ", km^-2)))+
  geom_sf(data = coast_sf, colour = "black", fill = "grey80")+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+  
  theme_minimal()+
  labs(title = paste(dfo_data$name, "mean predicted CPUE by Atlantis box (2003-2019) - stage:", dfo_data$stage, sep = " "))
```

Let's have a look at the variance per box over all years. We use the coefficient of variation, because CPUE varies widely between boxes.
```{r, fig.width = 10, fig.height = 5}
cv_all_years <- predictions_by_box %>% group_by(box_id, area, boundary) %>% summarise(cv = sd(mean_estimates)/mean(mean_estimates)) %>% ungroup()

ggplot()+
  geom_sf(data = cv_all_years[cv_all_years$box_id > 91,], aes(fill = cv))+ # log for visualisation
  scale_fill_viridis(name = "CV of CPUE")+
  geom_sf(data = coast_sf, colour = "black", fill = "grey80")+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+  
  theme_minimal()+
  labs(title = paste(dfo_data$name, "CV of predicted CPUE by Atlantis box (2003-2019) - stage:", dfo_data$stage, sep = " "))
```
A couple of boxes have a pretty high CV. This will change between species. 

Let's see how estimated CPUE changes over time, per box.
```{r, fig.width = 12, fig.height = 18}
predictions_by_box %>% 
  ggplot(aes(x = year,y = mean_estimates))+
  geom_point()+
  geom_line()+
  theme_minimal()+
  facet_wrap(~.bx0, scales = "free", ncol = 8)
```

Considerable variation over time. It may be worth assigning more weight to earlier years, although the distributions are supposed to be "generally representative" throughout the simulation, at least when it comes to S1-S4.

# Model skill

Trying to evaluate model skill by having a look at how well model predictions align with observations.

Since this is a spatially-explicit approach, we need observations and predictions at the same location. We use the locations of all RACE hauls as a prediction grid.   
```{r}
#make a prediction grid from the race data itself
dfo_grid_tmp <- dfo_data %>% dplyr::select(x, y, depth)

# add year
dfo_grid <- dfo_grid_tmp[rep(1:nrow(dfo_grid_tmp), length(all_years)),]
dfo_grid$year <- as.integer(rep(all_years, each = nrow(dfo_grid_tmp)))

# predict on this grid
predictions_at_locations <- predict(m_depth, newdata = dfo_grid, return_tmb_object = TRUE)
dfo_grid$predictions <- exp(predictions_at_locations$data$est) # back-transforming here
```

Now join by year and coordinates to have predictions at the sampling points. 
```{r, fig.width = 12, fig.height = 6}
dfo_corr <- dfo_data %>% left_join(dfo_grid, by = c("year", "y", "x"))
```

## Observed versus predicted

```{r}
paste0("Pearson's coef observations vs predictions: ", cor(dfo_corr$biom_kgkm2, dfo_corr$predictions, use = "everything", method = "pearson"))
```

Plot.
```{r, fig.width = 12, fig.height = 12}
ggplot(dfo_corr, aes(x = log1p(biom_kgkm2), y = log1p(predictions)))+ # log for visualisation
  geom_point(aes(color = depth.y))+
  scale_color_viridis()+
  geom_abline(intercept = 0, slope = 1)+
  theme_minimal()+
  facet_wrap(~year, scales = "free")+
  labs(title = paste(dfo_data$name, "observed vs predicted CPUE. Stage: ", dfo_data$stage, sep = " "))
```

These models often underpredict zeroes, i.e. they predict a catch where there was none. Does this happen randomly in space? Does it have a correlation of some kind with depth?

Plot zero catch from the data and the relative predictions. Turn to `sf` for plotting.
```{r, fig.width = 12, fig.height = 12}
dfo_corr %>% filter(biom_kgkm2 == 0) %>%
  mutate(x=x*1000,y=y*1000) %>%
  st_as_sf(coords = c(x = "x", y = "y"), crs = atlantis_bgm$extra$projection) %>%
  ggplot()+
  geom_sf(aes(color = log1p(predictions)))+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(dfo_data$x*1000)-35000,max(dfo_data$x*1000)), ylim=c(min(dfo_data$y*1000),max(dfo_data$y*1000)))+  
  scale_color_viridis()+
  theme_minimal()+
  labs(title = "Model predictions at zero-catch locations")+
  facet_wrap(~year, ncol = 3)
```

What about the relationship between model residuals and depth?
```{r, fig.width = 12, fig.height=16}
dfo_data %>%
  ggplot()+
  geom_point(aes(x = depth, y = resids, color = log1p(biom_kgkm2)))+
  geom_hline(yintercept = 0, color = "red", linetype = "dashed")+
  scale_color_viridis()+
  theme_minimal()+
  facet_wrap(~year, ncol = 2)
```

## Root Mean Square Error (RMSE)

Calculate RMSE between predicted and observed values.
```{r}
paste("RMSE:", sqrt(sum((dfo_corr$predictions - dfo_corr$biom_kgkm2)^2)/nrow(dfo_corr)), " kg km-2", sep = " ") ### traditional rmse metric, in units kg km2
```

Normalized RMSE. 
```{r}
rmse_cv <- sqrt(sum((dfo_corr$predictions - dfo_corr$biom_kgkm2)^2)/nrow(dfo_corr))/(max(dfo_corr$biom_kgkm2)-min(dfo_corr$biom_kgkm2))*100 #### normalised rmse, expressed as a % of the range of observed biomass values, sort of approximates a coefficient of variation 
paste("Normalised RMSE:", paste0(rmse_cv, "%"), sep = " ")
```

# Total biomass and biomass per box

The current estimated CPUE is in kg km$^{-2}$. So, just we just turn that into biomass per box. Remember that the area is in m$^2$ for the boxes, so need to divide by 1,000,000.
```{r}
means_all_years <- means_all_years %>% mutate(biomass = all_years_kgkm2*area*1e-06*1e-03) # from kg to t

means_canada <- means_all_years %>% filter(box_id> 91)
```

```{r}
means_canada %>% select(box_id, all_years_kgkm2, biomass) %>% st_set_geometry(NULL) %>% kable(align = 'lccc', format = "markdown", 
      col.names = c("Box", "CPUE (kg km-2)", "Biomass (mt)"))
```

Write out a .csv.
```{r}
out <- means_canada %>% st_set_geometry(NULL)

write.csv(x = out, 
          file = paste0("C:/Users/Alberto Rovellini/Documents/GOA/SDM/sdmTMB_Canada_stages/output/cpue_tables/",dfo_data$name[1],dfo_data$stage[1],"_DFO.csv"), 
          row.names = FALSE)
```

# Validation metrics

Let's produce a table that includes: convergence metrics; Pearson's correlation coefficient for predicted vs observed; RMSE; and normalized RMSE. 

```{r}
val <- data.frame(dfo_data$name[1], # group
                  m_depth$model$convergence, # convergence
                  m_depth$model$message, # more convergence
                  max(m_depth$gradients), # max gradient
                  tidy(m_depth, effects = 'ran_pars') %>% filter(term=='range') %>% pull(estimate), # matern range
                  cor(dfo_corr$biom_kgkm2, dfo_corr$predictions, use = "everything", method = "pearson"), # correlation
                  sqrt(sum((dfo_corr$predictions - dfo_corr$biom_kgkm2)^2)/nrow(dfo_corr)),# RMSE
                  sqrt(sum((dfo_corr$predictions - dfo_corr$biom_kgkm2)^2)/nrow(dfo_corr))/(max(dfo_corr$biom_kgkm2)-min(dfo_corr$biom_kgkm2))*100 # NRMSE
) %>% set_names(c("Group","Convergence","Message","Max gradient","Practical range (km)","Pearson's correlation","RMSE","NRMSE(%)"))

val

write.csv(x = val, 
          file = paste0("C:/Users/Alberto Rovellini/Documents/GOA/SDM/sdmTMB_Canada_stages/output/validation_tables/",dfo_data$name[1],dfo_data$stage[1],"_DFO.csv"), 
          row.names = FALSE)
```
